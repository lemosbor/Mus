from bs4 import BeautifulSoup # импорт класса BeautifulSoup из модуля bs4
import requests # модуль для веб-запросов
import json # модуль для работы с json
import re # модуль для работы с текстом
import webbrowser # модуль для работы с ссылками
import time # модуль для работы со временем
import mouse # модуль для имитации мышки
from fp.fp import FreeProxy # модуль выгрузки прокси https://pypi.org/project/free-proxy/
# ПАРСИНГ ЗАПИСЕЙ
конец=0 # индикатор окончания просмотра страниц
итерация=1 # первая страница
while конец == 0: # цикл перебора страниц
	try: # пробуем парсить (если сработает прокси)
		if итерация == 1: адрес ="https://funkysouls.org/music/index.html" # для первой страницы		
		else: адрес =("https://funkysouls.org/music/page/"+str(итерация)+".html") # для последующих		
		база = json.loads(open ('музыка.json', "r", encoding='utf-8').read()) # открываем базу
		альбомы = [i['а'] for i in база] # создаем перечень существующих записей (альбомов)
		предбаза = [] # создаем пустую предбазу
		проксиадрес = FreeProxy(rand=True).get() # выгружаем рабочий прокси
		запрос = requests.get(адрес, proxies={'http': проксиадрес, 'https': проксиадрес}) # делаем запрос к сайту, используя прокси
		запрос.encoding = 'utf-8' #перекодируем результат запроса для корректного отображения кириллици
		суп = BeautifulSoup(запрос.text,  'lxml') # создание объекта BeautifulSoup (суп) и передача его конструктору. Вторая опция уточняет объект парсинга.
		массив = суп.find_all("h2") # из супа формируем массив по тегу h2
		альбом = [м.text for м in массив if м.text is not None] # формируем список названий (text) из массива
		массивссылок = суп.find_all("p", class_="download_box") # из супа формируем массив ссылок по тегу р с классом download_box
		ссылки = [м.a['href'] for м in массивссылок if м.a['href'] is not None] # формируем список ссылок (элемент href) из массива ссылок
		for i, n in zip(альбом, ссылки): # объединение списков названий и ссылок по ключам
			предбаза.append({'а' : i, 'ф' : n}) # добавляем в подбазу название альбома (а) и ссылку на форум (ф)
		предбаза = [i for i in предбаза if not (i["а"] in альбомы)] #удаляем из предбазы существующие альбомы (из перечня альбомы)
		база = база + предбаза # добавляем записи в базу
		with open("музыка.json", "w", encoding='utf-8') as i: i.write(json.dumps(база, ensure_ascii = False)) # записываем базу
		print("Добавленно альбомов:", len(предбаза))
		итерация +=1
		if len(предбаза) < 14: конец = 1 # если записалось меньше 14 альбомов, то мы достигли конца и пора прекращать цикл
	except: print("Не удолось подключиться с прокси:", проксиадрес, "Продолжаем.")
# ПАРСИНГ ФОРУМА (поиск ссылок на файлы-архивы)
база = json.loads(open ('музыка.json', "r", encoding='utf-8').read()) # открываем базу
for i in база: # прогоняем по всем записям
	if "ссылка" not in i: # если нет ссылки на файл-архив
		адрес = i['ф'] # адрес — ссылка на форум
		назв = i['а'] # название альбома
		print("Парсим форум по альбому:", назв)
		try: # пробуем парсить (если сработает прокси)
			запрос = requests.get(адрес, proxies={'http': проксиадрес, 'https': проксиадрес}) # делаем запрос к адресу, используя прокси
			запрос.encoding = 'utf-8' #перекодируем результат запроса
			суп = BeautifulSoup(запрос.text,  'html.parser') # создание объекта BeautifulSoup (суп) и передача его конструктору	
				# Способ 1
			номер = "p"+(адрес.split('p/')[1]) # номер сообщения на форуме (индекс «р» + часть адреса. разделение по индексу p/)	
			сообщение = суп.find("div", id=номер) # находим код с тегом div и id номера сообщения
			ссылка = сообщение.find('div', class_='text').find_all(string=re.compile('zippyshare.com')) # в сообщении находим тег div с классом text, а в нём находим ссылку по ключевому слову		
			if len(ссылка) > 0:
				ссылка = ссылка[0]
				print("Найденная по Способу 1 ссылка:", ссылка) 
				continue # останавливаем и переходим к следующему i
			print("Способ 1 не сработал. Ищем по способу 2")
			# Способ 2
			файлообменники = ["zippyshare.com", "yadi.sk", "cloud.mail"]
			for k in файлообменники:
				позиция0=(str(суп).rfind(k)) #re.findall("......zippyshare.com....", суп)[-1]
				if позиция0 > 0:
					print("Ссылка найдена в позиции:", позиция0),
					смещение = 0
					if k == "zippyshare.com": смещение = 7
					подсуп = str(суп)[позиция0:]-смещение # обрезаем код страници с позиции1
					подсуп = re.sub('[#% !@\[*\]|"<?]','Ж',подсуп)
					позиция2=подсуп.find("Ж") # позиция первого словосочетания в подсупе(окончание адреса ссылки)			
					ссылка = подсуп[:позиция2] # обрезаем обрезанный код до позиции2
					print("вот она:", ссылка)
					break	 
			i.update({'ссылка' : ссылка}) # записываем ссылку
			#i.update({'жанр' : [] }) # записываем список жанров
			#for жанр in суп.find_all("a", class_="ftag"): # находим все жанры
			#	i["жанр"].append(жанр.text) # записываем все жанры в список жанров
			#спасибо = суп.find("div", id=номер).find('p', class_='thanx') # поиск количества сказавших спасибо ДОРАБОТАТЬ
			#i.update({'р' : (спасибо.findChildren()[2].text)}) #записываем количество сказавших спасибо
			with open("музыка.json", "w", encoding='utf-8') as i: i.write(json.dumps(база, ensure_ascii = False)) # ЗАПИСЫВАЕМ базу
		except:
			проксиадрес = FreeProxy(rand=True).get() # выгружаем рабочий прокси
			print("Не удолось подключиться с прокси:", проксиадрес, "Продолжаем.")
# ОТКРЫВАТЕЛЬ
print("Парсинг завершён. Приступаем к открытию ссылок.")
база = json.loads(open ('музыка.json', "r", encoding='utf-8').read()) # открываем базу
for i in база: # для записи в массиве записей
	if "ссылка" in i and "скачен" not in i: # если в записи есть ссылка, но нет индекса «скачен»
		webbrowser.open(i['ссылка']) # открыть ссылку в браузере
		time.sleep(4) # подождать 4 сек
		mouse.move(900,  300, absolute=True, duration=0.2) # перевести указатель мыши в позицию 900, 300
		mouse.click('left') # левый клик мыши
		i.update({'скачен' : 1}) # записать индекс "скачен"
		print(i['а'], "ссылка открыта. Дан тег «скачен».")
	with open("музыка.json", "w", encoding='utf-8') as i: i.write(json.dumps(база, ensure_ascii = False)) # записываем базу
